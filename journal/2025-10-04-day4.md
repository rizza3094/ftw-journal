# Journal — 2025-10-04 — Data Engineering Day 4

## 1) What I learned (bullets, not prose)
- Data transformation can both retain and reshape data, but not delete it.
- Importance of problem framing before jumping into solutioning.
- Recognized that there are always trade-offs — e.g., speed vs understandability, automation vs control.
- Learned to separate core business questions (make/save money, compliance) from sub-questions (operational details).
- Realized that every pipeline or model contributes to long-term infrastructure — it becomes part of your organization’s data legacy.
- Data engineering requires thinking about future maintainability and clarity as much as immediate performance.

## 2) New vocabulary (define in your own words)
- Problem Framing – Clearly defining what needs solving before jumping into how to solve it.
- Trade-off – The balance between two desirable but conflicting factors (e.g., speed vs accuracy).
- Data Transformation – Changing or restructuring data to make it more useful or ready for analysis, without losing the original integrity.
- Legacy Infrastructure – The systems and data architecture that remain and evolve over time, influencing future work.

## 3) Data Engineering mindset applied (what principles did I use?)
- Evaluated trade-offs before deciding on tools or data structures.

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- Decision: Retain raw data and transform through new tables or columns.
    Why: To preserve source integrity and enable reproducibility.
    Alternative: Overwriting existing data (faster but riskier).
    Trade-off: More storage use vs data safety.
- Decision: Prioritize understanding the problem before selecting tools.
    Why: Prevents misaligned solutions.
    Trade-off: More upfront time vs smoother implementation later.

## 5) Open questions (things I still don’t get)
- When is it safe or necessary to overwrite data instead of retaining all history?
How do I decide which column should be the primary key when multiple columns could uniquely identify a record?
- What are the best practices for creating foreign key relationships in large datasets (e.g., performance vs referential integrity)?
- How can I verify if my foreign keys are properly enforcing relationships across tables in real-world data pipelines?
- In modern data warehouses like Snowflake or BigQuery, are foreign key constraints always necessary, or can they be handled logically in transformations?
- What are the risks of not enforcing primary and foreign keys in analytical data models?

![Alt text](../assets/b.jpg "?")

## 6) Next actions (small, doable steps)

- Time management
- Include one SQL DataCamp Course in my daily Schedule
- Document learnings weekly using this reflection template.
- Explore data transformation best practices in dbt

## 7) Artifacts & links (code, queries, dashboards)

https://www.hackerrank.com/domains/sql

### Mini reflection (3–5 sentences)
What surprised me? What would I do differently next time? What will I watch out for in production? 

I didn’t expect to spend so much time just trying to understand the problem before touching any data, but it really paid off. It reminded me how important it is to slow down and frame things clearly from the start. Next time, I want to be more intentional about balancing performance with clarity, and in production, I’ll make sure to watch for silent data overwrites and keep my transformation steps well-documented.

### BONUS: What is a meme that best describes what you feel or your learning today?

![Alt text](../assets/c.jpg "data?")